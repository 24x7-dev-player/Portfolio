<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tushar-Perspective</title>
    <link rel="shortcut icon" href="../images/tushar.png" type="image/x-icon">
    <link rel="stylesheet" href="../style.css">
    <link href='https://fonts.googleapis.com/css?family=JetBrains Mono' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Space Grotesk' rel='stylesheet'>







</head>

<body>
    <div class="video-container">
        <video autoplay loop muted id="background-video">
            <source src="../images/video8.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </div>


    <div class="front">
        <span>Created By <a style="color: orange;" target="_blank"
                href="https://tushar-perspective.github.io/Portfolio/">TUSHAR-PERSPECTIVE</a> | <span
                class="far fa-copyright"></span>
            2024 All rights
            reserved.</span>

    </div>




    <!---INFO---------->
    <section class="info2" id="ab">
        <div class="max-width">
            <h2 class="titles">RAG :Retrieval Augmented
                Generation (RAG) <span style="color: #BDA3A1;"> :Pipeline</span>
                <a style=" color: inherit; " href="https://github.com/Thinkliketushar" target="_blank"> </a>
            </h2>
            <div class="info-content">
                <div class="column right">

                    <h1>INTRODUCTION</h1>
                    <ul>
                        <li>RAG: Smarter LLMs with External Knowledge</li>
                        <ul>
                            <li>Boosts Large Language Models (LLMs) by incorporating external information.</li>
                            <li>Benefits:</li>
                            <ul>
                                <li>More Accurate & Complete Answers (utilizes real-world knowledge)</li>
                                <li>Reduced Hallucination Risks (verifiable sources enhance trust)</li>
                                <li>Better Out-of-Distribution Performance (handles unexpected queries well)</li>
                                <li>Easier to Use with Different LLMs (works seamlessly with various models)</li>
                            </ul>
                            <li>How it Works:</li>
                            <ul>
                                <li>Finds Relevant Information (searches external sources to answer your query)</li>
                                <li>Informs the LLM (LLM uses this knowledge alongside its own to generate a response)
                                </li>
                            </ul>
                            <li>Think of RAG as a knowledgeable assistant for LLMs, providing them with the right
                                context for better results.</li>
                        </ul>
                    </ul>

                    <h1>RAG Applications in Business:</h1>
                    <ul>
                        <li>Customer Question-Answering Systems:</li>
                        <ul>
                            <li>
                                RAG-enhanced chatbots can answer customer queries in various domains (healthcare,
                                manufacturing, IT) by retrieving relevant information from internal knowledge bases.
                            </li>
                            <li>
                                Example: A healthcare organization uses RAG models to access and interpret medical
                                literature or internal trial records to provide accurate medical answers.
                            </li>
                        </ul>
                        <li>Content Recommendation Systems:</li>
                        <ul>
                            <li>
                                RAG improves user interaction and content engagement in recommendation systems by using
                                advanced retrievers for personalized recommendations.
                            </li>
                        </ul>
                        <li>Educational and Legal Research Analysis:</li>
                        <ul>
                            <li>
                                RAG simplifies and condenses study materials in education and accesses and summarizes
                                relevant legal materials for legal analysis and case preparation.
                            </li>
                        </ul>
                        <li>Content Creation and Summarization:</li>
                        <ul>
                            <li>
                                RAG models assist with content creation by finding useful information from various
                                sources, including internal knowledge bases. They can help create condensed reports and
                                summaries.
                            </li>
                            <li>
                                Example: Writers and researchers can use internal RAG models with a news repository to
                                create news articles or summaries of long reports.
                            </li>
                        </ul>
                    </ul>

                    <h1>Future Developments in RAG:</h1>
                    <ul>
                        <li>Pluggable and Modular Search Methods: Advanced and modular RAG approaches for flexible
                            search methods.</li>
                        <li>Multi-modal RAG: Integration of different data types (text, images, etc.) for a richer
                            understanding.</li>
                        <li>Simplified Retrieval: Streamlining retrieval methods, especially during multi-hop retrieval
                            and re-ranking for efficiency.</li>
                        <li>Production-Ready RAG: Making RAG models more practical and deployable for real-world
                            applications.</li>
                        <li>Long-Context RAG: Enabling RAG to handle and leverage longer contextual information.</li>
                    </ul>

                    <h1> Pros and Cons of RAG:</h1>
                    <ol>Pros:</ol>
                    <ul>
                        <li>Real-time insights for relevant responses.</li>
                        <li>Works with diverse knowledge sources.</li>
                        <li>Empowers decision-making with external info.</li>
                    </ul>
                    <ol>Cons:</ol>
                    <ul>
                        <li>Maintaining data sources and scalability can be challenging.</li>
                        <li>Sophisticated algorithms increase implementation complexity.</li>
                    </ul>

                    <h1> Pros and Cons of Finetuing:</h1>
                    <ol>Pros:</ol>
                    <ul>
                        <li>Customizes models for specific tasks, saving time.</li>
                        <li>Improves accuracy and performance for specific domains.</li>
                        <li>Enables models to learn and adapt to new data.</li>
                    </ul>
                    <ol>Cons:</ol>
                    <ul>
                        <li>Risk of amplifying biases from training data.</li>
                        <li>Requires significant computational resources and expertise.</li>
                    </ul>



                    <h1>Challenges and Solutions in Naive RAG:</h1>

                    <ol>Challenges:</ol>
                    <ul>
                        <li>Incomplete or irrelevant information retrieval.</li>
                        <li>Difficulty integrating context from retrieved chunks.</li>
                        <li>LLM generation issues (not grounded in context).</li>
                        <li>Inaccurate semantic representations and low-quality retrieved chunks.</li>
                        <li>Difficulties with capturing complex relations and finding diverse information.</li>
                        <li>Optimizing data indexing for retrieval efficiency.</li>
                    </ul>

                    <ol>Solutions:</ol>
                    <ul>
                        <li>Improve data quality (remove irrelevant info, clarify entities, etc.).</li>
                        <li>Optimize index structure (chunk size, relationship information).</li>
                        <li>Add metadata to chunks (dates, chapters, purposes, etc.).</li>
                        <li>Optimize chunk size based on document types.</li>
                        <li>Pre-retrieval techniques (Sliding Window, Auto-Merging, etc.).</li>
                        <li>Domain knowledge fine-tuning for embedding models.</li>
                        <li>Choose appropriate similarity metrics (Cosine Similarity, etc.).</li>
                        <li>Reranking retrieved information for relevance.</li>
                        <li>Prompt compression (reduce context length and remove irrelevant information).</li>
                    </ul>

                    <h1> Basic Rag Pipeline:</h1>
                    <ul>
                        <li><strong>Document Loader and Text Processing:</strong>
                            <ul>
                                <li>Load documents from various sources.</li>
                                <li>Preprocess text by cleaning and standardizing it.</li>
                            </ul>
                        </li>
                        <li><strong>Chucking Documents:</strong>
                            <ul>
                                <li>Segment long documents into smaller chunks.</li>
                                <li>Choose chunking strategies like fixed-size or sentence-based.</li>
                            </ul>
                        </li>
                        <li><strong>Embedding of Chucked Documents:</strong>
                            <ul>
                                <li>Convert each document chunk into a numerical vector representation.</li>
                            </ul>
                        </li>
                        <li><strong>Storing Embeddings in Vector Database:</strong>
                            <ul>
                                <li>Store the generated document embeddings in a specialized database for efficient
                                    similarity comparisons.</li>
                            </ul>
                        </li>
                        <li><strong>Retrieval:</strong>
                            <ul>
                                <li>Given a user's query, find the most relevant document chunks from the database.</li>
                                <li>Compare query embedding to stored document embeddings using similarity metrics.</li>
                                <li>Rank retrieved chunks based on relevance factors.</li>
                            </ul>
                        </li>
                        <li><strong>Summarization:</strong>
                            <ul>
                                <li>Provide a humanized response based on the user's query and retrieved information.
                                </li>
                            </ul>
                        </li>
                        <li><strong>Evaluation:</strong>
                            <ul>
                                <li>Evaluate the system's performance in terms of retrieval and generative quality using
                                    metrics like Hit Rate, MRR, Bleu score, and Rouge score.</li>
                            </ul>
                        </li>
                    </ul>

                    <h1>Advanced RAG Pipeline:</h1>
                    <ul>
                        <li>Query Decomposition - Break down questions and reformulate for better understanding.</li>
                        <li>Pseudo Document - Create hypothetical responses to improve search quality.</li>
                        <li>Rewrite Retrieve Read (RRR) - Rewrite query, retrieve documents, and generate answer.</li>
                        <li>Routing - Direct queries to appropriate indexes or use LLM to identify data type.</li>
                        <li>Metadata Filtration - Use document metadata to narrow down search results.</li>
                        <li>Optimized Indexing - Utilize multiple representations and hierarchical indexing for broader
                            search.</li>
                        <li>Fine-tune Embedding and LLMs - Adjust models for better understanding of specific contexts.
                        </li>
                        <li>Reranking - Re-order retrieved documents to prioritize relevant and diverse contexts.</li>
                        <li>Active Retrieval - Evaluate answers, identify gaps, and retrieve additional information.
                        </li>
                        <li>Self-RAG - Conduct multiple retrieval rounds based on LLM assessment and use a "Critique
                            Model" to refine retrieval.</li>
                    </ul>

                    <ul>
                        <li>Late-stage interaction: Efficient document retrieval through precomputed embeddings for good
                            balance between computation cost and quality.</li>
                        <li>Hierarchical Indexing with RAPTOR: Tree structure of summaries at various levels for
                            accurate retrieval, capturing larger context for thematic comprehension and granularity.
                            (Source: Arxiv)</li>
                        <li>Corrective Retrieval Augmented Generation (CRAG): Assesses and corrects retrieved documents,
                            retrieves additional information for improved answer accuracy. [ paper | code ]</li>
                    </ul>

                    <h1>RAG Advantages Over Fine-tuning LLMs</h1>
                    <ul>
                        <li>Fine-Tuning LLMs (Downsides):
                            <ul>
                                <li>Forgetting: Loses pre-training skills (e.g., finance-tuned LLM struggles with
                                    general conversation).</li>
                                <li>Data Dependence: Relies on large, high-quality training data (expensive to collect).
                                </li>
                                <li>Limited Knowledge: Restricted to information in training data, lacks real-world
                                    knowledge.</li>
                                <li>Inflexible: Requires expensive retraining for any changes.</li>
                            </ul>
                        </li>
                        <li>RAG Systems (Advantages):
                            <ul>
                                <li>Preserves Capabilities: Retains skills from pre-training (LLM itself isn't
                                    modified).</li>
                                <li>External Knowledge: Integrates customizable external knowledge sources (databases).
                                </li>
                                <li>Adaptable Knowledge: Changes knowledge sources without retraining LLM.</li>
                                <li>Lower Data Needs: Requires less data since LLM isn't retrained.</li>
                            </ul>
                        </li>
                    </ul>

                    <h1>When to Fine-Tune vs RAG for Different
                        Model Sizes</h1>
                    <ul>
                        <li>Fine-Tuning vs RAG for Different Model Sizes:
                            <ul>
                                <li>Large Language Models (LLMs): Prefer RAG for LLMs (e.g., GPT-4) to:
                                    <ul>
                                        <li>Retain pre-training skills (conversation, translation, etc.)</li>
                                        <li>Utilize external knowledge (databases)</li>
                                        <li>Avoid catastrophic forgetting (damaging abilities)</li>
                                        <li>Have flexible knowledge sources (change without retraining)</li>
                                    </ul>
                                </li>
                                <li>Medium Language Models (MLMs): Both RAG and fine-tuning are options for MLMs (e.g.,
                                    Llama 2):
                                    <ul>
                                        <li>Fine-tuning for memorization (question answering)</li>
                                        <li>RAG for specific tasks (retrieving relevant knowledge)</li>
                                    </ul>
                                <li>Choose based on needing the MLM's full general knowledge.</li>
                        </li>
                        <li>Small Language Models (SLMs): Fine-tuning is better for SLMs (e.g., Zephyr) because:
                            <ul>
                                <li>They lack general capabilities of larger models</li>
                                <li>Fine-tuning directly infuses knowledge</li>
                                <li>Less risk of catastrophic forgetting</li>
                                <li>Easier to retrain with new data</li>
                            </ul>
                        </li>
                    </ul>
                    </li>

                    <h1>Using RAG and Fine-tuning for Pre-
                        trained Models</h1>
                    <li>RAG vs Fine-Tuning for Pre-trained Models: Both are applicable for pre-trained models (LLMs or
                        custom models).
                        <ul>
                            <li>RAG for Pre-Trained Models: Use RAG when you want to:
                                <ul>
                                    <li>Leverage general knowledge (retain conversation, analysis, etc.)</li>
                                    <li>Minimize forgetting (avoid damaging general abilities)</li>
                                    <li>Utilize external knowledge (augment with retrieved knowledge)</li>
                                    <li>Have flexible knowledge (swappable databases for changing knowledge needs)</li>
                                </ul>
                            </li>
                            <li>Fine-Tuning Pre-Trained Models: Fine-tuning is better when you want to:
                                <ul>
                                    <li>Have a specialized focus (task relies heavily on specific knowledge)</li>
                                    <li>Train for memorization (e.g., memorize customer profiles)</li>
                                    <li>Have static knowledge (knowledge requirements are fixed)</li>
                                    <li>Use a small model (fine-tuning directly infuses knowledge with less risk)</li>
                                </ul>
                            </li>
                        </ul>
                    </li>
                    </ul>


                </div>
            </div>
    </section>
    <!----------------->










    <script src="script.js"></script>
</body>



</html>