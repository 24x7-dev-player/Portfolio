<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Tushar-Perspective</title>
    <link rel="shortcut icon" href="../images/tushar.png" type="image/x-icon">
    <link rel="stylesheet" href="../style.css">
    <link href='https://fonts.googleapis.com/css?family=JetBrains Mono' rel='stylesheet'>
    <link href='https://fonts.googleapis.com/css?family=Space Grotesk' rel='stylesheet'>







</head>

<body>
    <div class="video-container">
        <video autoplay loop muted id="background-video">
            <source src="../images/video8.mp4" type="video/mp4">
            Your browser does not support the video tag.
        </video>
    </div>



    <div class="front">
        <span>Created By <a style="color: orange;" target="_blank"
            href="https://tushar-perspective.github.io/Portfolio/">TUSHAR-PERSPECTIVE</a> | <span
            class="far fa-copyright"></span>
        2024 All rights
        reserved.</span>

    </div>



    <!---INFO---------->
    <section class="info2" id="ab">
        <div class="max-width">
            <h2 class="titles">Transformers<span style="color: #BDA3A1;"> :Architecture</span>
                <a style=" color: inherit; " href="https://github.com/Thinkliketushar" target="_blank"> </a>
            </h2>
            <div class="info-content">
                <div class="column right">

                    <h1>INTRODUCTION</h1>
                    <ul>
                        <li>Transformer Architecture: Relies on a series of transformations (like paying attention to
                            important parts of a sentence).</li>
                        <li>Key functionalities:
                            <ul>
                                <li>Pays attention to important words in a sentence</li>
                                <li>Understands context by looking at all words together</li>
                                <li>Weighs relationships between words</li>
                                <li>Combines insights to understand the whole sentence</li>
                                <li>Predicts next words</li>
                            </ul>
                        </li>
                        <li>Self-attention Mechanism: Allows the model to consider all positions in the sentence
                            simultaneously</li>
                        <li>Parallelization: Enables faster training and inference compared to sequential models</li>
                        <li>Long-range Dependencies: Excels at capturing relationships between distant words in the
                            sequence</li>
                        <li>Positional Encoding: Incorporates information about the position of words in the sequence
                        </li>
                        <li>Encoder-Decoder Architecture: Effective for sequence-to-sequence tasks like machine
                            translation</li>
                        <li>Transfer Learning: Benefits from being pre-trained on large amounts of text data</li>
                    </ul>

                    <h1>Pre-processing (Tokens, Word Embedding and
                        Positional Encoding):</h1>
                    <ul>
                        <li>Tokenization: Words are converted into numerical representations (tokens).</li>
                        <li>Word Embeddings: Each token is assigned a vector representing its meaning and relationships
                            to other words.</li>
                        <li>Positional Encoding: Additional information about a word's position in the sentence is added
                            to its embedding.
                            <ul>
                                <li>Reason: Transformers process words in parallel, so they don't inherently understand
                                    word order like other models. Positional encoding addresses this.</li>
                            </ul>
                        </li>
                    </ul>

                    <h1>Self-Attention Layer:</h1>
                    <ul>
                        <li>Captures relationships between words in a sequence</li>
                        <li>Determines meaning of a word based on other words in the sentence</li>
                        <li>Process:
                            <ul>
                                <li>Creates three vectors for each word:</li>
                                <ul>
                                    <li>Query (Q): Represents the current word in focus</li>
                                    <li>Key (K): Represents other words relevant to the Query</li>
                                    <li>Value (V): Contains information from relevant Key words</li>
                                </ul>
                                <li>Applies linear transformation to word embedding and positional encoding</li>
                                <li>Creates Q, K, and V vectors for each word</li>
                            </ul>
                        </li>
                        <li>Example:
                            <ul>
                                <li>Sentence: "Hi how are you?"</li>
                                <li>Q1: "Hi" ; K1, V1: ("how", "are", "you")</li>
                                <li>(Process repeats for each word, shifting focus to the next word as a Query)</li>
                            </ul>
                        </li>
                    </ul>

                    <h1>Linear Projections:</h1>
                    <ul>
                        <li>Linear Projections:
                            <ul>
                                <li>Shrink word embeddings (dense tensors) & create Q/K/V vectors.</li>
                            </ul>
                        </li>
                        <li>Attention Scores:
                            <ul>
                                <li>Show how relevant words are to each other (based on Q & K).</li>
                            </ul>
                        </li>
                        <li>Weighted Sum:
                            <ul>
                                <li>Combines context for each word based on attention scores.</li>
                            </ul>
                        </li>
                    </ul>

                    <h1>The Softmax function:</h1>
                    <ul>
                        <li>Softmax Function: Converts model output into probabilities (used for attention weights).
                        </li>
                        <li>Attention Mechanism:
                            <ul>
                                <li>Scaled Dot Product: Scores how relevant other words are to a specific word.</li>
                                <li>Attention Weights: Determine how much a word should focus on other words.</li>
                                <li>Multiple Attention Heads: Allow the model to focus on different aspects of the
                                    sentence simultaneously (like people in a discussion).
                                    <ul>
                                        <li>Multi-Head Attention: Uses multiple Scaled Dot Product layers in
                                            parallel.
                                            <ul>
                                                <li>Compares a word to all other words at once (increased speed).</li>
                                                <li>Context Vector: Combines outputs from all attention heads to
                                                    represent the word's meaning in context.</li>
                                            </ul>
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>Output: Contextualized word representations considering their relationships.</li>
                        <li>Feed-Forward Layer: Extracts additional features and non-linear patterns from the data.
                        </li>
                    </ul>

                    <h1>Feed Forward Networks:</h1>
                    <ul>
                        <li>Feed-Forward Layer (FFN):
                            <ul>
                                <li>Extracts additional features, finds deeper meanings/connections, learns non-linear
                                    patterns.</li>
                                <li>Process:
                                    <ul>
                                        <li>Linear Transformation:** Projects input to a different space.</li>
                                        <li>Activation Function (ReLU): Introduces non-linearity.</li>
                                        <li>Second Linear Transformation: Refines the representations.</li>
                                        <li>Residual Connection: Combines original and refined information.</li>
                                        <li>Normalization: Standardizes values for faster learning and adaptation.
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                    </ul>

                    <h1>Summery:</h1>
                    <ul>
                        <li>Input Processing:
                            <ul>
                                <li>Words: Split into individual units.</li>
                                <li>Tokenization: Convert words to numerical representations (tokens).</li>
                                <li>Word Embeddings + Positional Encoding: Add meaning and position information to
                                    tokens.</li>
                                <li>Linear Transformations: Reduce dimensionality and prepare tokens for comparison.
                                </li>
                            </ul>
                        </li>
                        <li>Encoder:
                            <ul>
                                <li>Self-Attention Layer: Captures relationships between words.</li>
                                <ul>
                                    <li>Calculates attention scores (Q, K, V vectors).</li>
                                    <li>Softmax normalizes scores (0-1).</li>
                                    <li>Generates context vector for word meaning.</li>
                                </ul>
                                <li>Feed-Forward Network: Extracts deeper features and patterns.</li>
                            </ul>
                        </li>
                        <li>Overall Process: Encoded tokens are used for various tasks.</li>
                    </ul>



                    <h1>Model Pretraining:</h1>
                    <ul>
                        <li>Model Pre-Training: Different models are trained for different purposes using various
                            techniques.</li>
                        <li>
                            <ul>
                                <li>Autoencoding Models (Encoder-Only):
                                    <ul>
                                        <li>Train by masking words and predicting them (reconstructing the sentence).
                                        </li>
                                        <li>Good for understanding relationships between words.</li>
                                    </ul>
                                </li>
                                <li>Autoregressive Models (Decoder-Only):
                                    <ul>
                                        <li>Train by predicting the next word based on previous words (causal language
                                            modeling).</li>
                                        <li>Good at generating text one word at a time.</li>
                                    </ul>
                                </li>
                                <li>Sequence-to-Sequence Models (Encoder-Decoder):
                                    <ul>
                                        <li>Use both encoder and decoder for understanding and generating text.</li>
                                        <li>Used for translation, summarization, question answering (e.g., BART model).
                                        </li>
                                    </ul>
                                </li>
                            </ul>
                        </li>
                        <li>Choosing a Model: The best model depends on the specific task.</li>
                    </ul>


                    <h1>What Matter Most:</h1>
                    <ul>
                        <li>

                            Key Factors for LLM Performance:</li>
                        <ul>
                            <li>Model Size (More Parameters): Like giving a model more muscles, this can improve
                                performance but increases training cost.</li>
                            <li>Training Dataset Size (More Material): More training data improves performance, with an
                                optimal size of about 20x model parameters.</li>
                            <li>Training Compute Power (More Horsepower): More compute power improves performance but is
                                expensive.</li>
                        </ul>
                        <li>Trade-offs:</li>
                        <ul>
                            <li>Bigger models require more data and compute, increasing training cost.</li>
                            <li>Research suggests current models might be over-parameterized and under-trained on data.
                            </li>
                        </ul>
                        <li>Additional Costs:</li>
                        <ul>
                            <li>Inference Cost: Running the LLM for a response.</li>
                            <li>Tuning Cost: Fine-tuning a pre-trained model for specific tasks.</li>
                            <li>Pre-training Cost: Training a new LLM from scratch.</li>
                            <li>Hosting Cost: Deploying and maintaining the LLM.</li>
                        </ul>
                        <li>Choosing the Right Mix: The best approach depends on available resources and cost
                            considerations.</li>
                    </ul>

                    <h1>Benefits and shortcomings of Small Language
                        Models:</h1>
                    <ul>
                        <li>Benefits:</li>
                        <ul>
                            <li>Efficient: Require less computational power, making them easier to deploy.</li>
                            <li>Cost-effective: Less expensive to train, maintain, and run compared to LLMs.</li>
                            <li>Specialized: Trained on specific tasks for better performance in that domain.</li>
                            <li>Explainable: More transparent outputs due to simpler models and targeted data.</li>
                        </ul>
                        <li>Shortcomings:</li>
                        <ul>
                            <li>Task-limited: Struggle with tasks outside their training domain (lack general
                                knowledge).</li>
                            <li>Performance-limited: Lower capacity for learning complex language patterns.</li>
                            <li>Dataset-dependent: Reliant on high-quality, relevant training data for robustness.</li>
                        </ul>
                    </ul>

                    <h1>Customizing Language Models for
                        Specialized Domains:</h1>
                    <ul>
                        <li>Domain Adaptation: LLMs struggle with specialized fields due to unique language.</li>
                        <li>
                            <ul>
                                <li>Challenge: Law, medicine, etc. have specific vocabulary and structures.</li>
                                <li>Solution: Adapt LLMs to these domains for better performance.</li>
                            </ul>
                        </li>
                        <li>Example: BloombergGPT (Finance): Trained on both financial data and general text for better
                            finance performance.</li>
                    </ul>


                    <h1>Quantization for NLP Models:</h1>
                    <ul>
                        <li>Reduces memory footprint: Stores model weights with lower precision (e.g., 8-bit integers)
                            instead of full 32-bit floats.</li>
                        <li>Process: Projects original weights into lower precision spaces using scaling factors.</li>
                        <li>Savings:
                            <ul>
                                <li>FP16 (half precision): Reduces memory by 50% (from 32-bit).</li>
                                <li>INT8 (integers): Reduces memory by 98.75% (from 32-bit).</li>
                            </ul>
                        </li>
                        <li>BFLOAT16 (BF16): A hybrid format between FP16 and FP32, capturing full dynamic range in 16
                            bits.</li>
                    </ul>

                    <h1>Scaling Techniques for Large Models:</h1>
                    <ul>
                        <li>Distributed Data-Parallel (DDP): Distributes data across multiple GPUs for parallel
                            processing.</li>
                        <li>Fully Sharded Data Parallel (FSDP): Splits model across multiple GPUs for models exceeding
                            single-GPU memory.</li>
                        <li>
                            ZeRO Memory Optimization: Shards optimizer states, gradients, and model parameters across
                            GPUs for further memory reduction.
                            <ul>
                                <li>Stage 1: Shards optimizer states, reducing memory usage by up to a factor of four.
                                </li>
                                <li>Stage 2: Shards gradients, further reducing memory usage by up to eight times when
                                    combined with Stage 1.</li>
                                <li>Stage 3: Shards all components, including model parameters, with memory reduction
                                    scaling linearly with the number of GPUs.</li>
                            </ul>
                        </li>
                    </ul>




                </div>
            </div>
    </section>
    <!----------------->










    <script src="script.js"></script>
</body>



</html>